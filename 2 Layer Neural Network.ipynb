{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "- Added one more layer to the network and increased the hidden nodes. \n",
    "- I have implemented the neural network in such a way that it'll give us all the outputs using all the activation functions. \n",
    "- Firslty I have defined all the required activation functions like sigmoid, relu and tanh. \n",
    "- Secondly defined the model(forward and backward propagation) with all the activation functions, so that we can compare the results, which is best of all, which is overfitting, underfitting or the perfect fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "def sigmoid(z):\n",
    "    return (1.0/(1.0 + np.exp(-z)))\n",
    "\n",
    "def sigmoid_deravative(z):\n",
    "    return z * (1 - z)\n",
    "\n",
    "def tanh_function(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_deravative(z):\n",
    "    return (1-np.tanh(z)**2)\n",
    "\n",
    "def relu_function(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def relu_deravative(z): \n",
    "    return 1.0 * (z>0)\n",
    "\n",
    "def leaky_relu(self, z):\n",
    "    slope = 0.1\n",
    "    z = np.maximum(slope * z, z)\n",
    "    return z\n",
    "def Error_found(true_values, predicted):\n",
    "    result = predicted - true_values\n",
    "#     result = true_values - predicted\n",
    "    return result\n",
    "\n",
    "def leaky_relu_deravative(self, z):\n",
    "    slope = 0.1\n",
    "    leaky = np.zeros_like(z)\n",
    "    leaky[z<=0] = slope\n",
    "    leaky[z>0] = 1\n",
    "    return leaky\n",
    "\n",
    "class NeuralNetwork_model:\n",
    "    def __init__(self, X,y,lr,hiddenlayer, activation_function):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        hiddenlayer = 4\n",
    "        self.lr = 0.001\n",
    "        x_shape = X.shape[1]\n",
    "        self.w1 = np.random.randn(x_shape, hiddenlayer)  #(4*350) (350*4)\n",
    "        self.b1 = np.zeros(hiddenlayer) #(4*1)\n",
    "        self.w2 = np.random.randn(hiddenlayer, hiddenlayer)\n",
    "        self.b2 = np.zeros(hiddenlayer)\n",
    "        self.w3 = np.random.randn(hiddenlayer, 1) #(350*4)\n",
    "        self.b3 = np.zeros(1)\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        self.x = x\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            z1 = np.dot(self.x, self.w1) + self.b1\n",
    "            self.a1 = sigmoid(z1)\n",
    "            z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "            self.a2 = sigmoid(z2)\n",
    "            z3 = np.dot(self.a2, self.w3) + self.b3\n",
    "            self.a3 = sigmoid(z3)\n",
    "            return self.a3\n",
    "        \n",
    "        elif self.activation_function == \"relu\":\n",
    "            z1 = np.dot(self.x, self.w1) + self.b1\n",
    "            self.a1 = relu_function(z1)\n",
    "            z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "            self.a2 = relu_function(z2)\n",
    "            z3 = np.dot(self.a2, self.w3) + self.b3\n",
    "            self.a3 = relu_function(z3)\n",
    "            return self.a3\n",
    "        \n",
    "        elif self.activation_function == \"tanh\":\n",
    "            z1 = np.dot(self.x, self.w1) + self.b1\n",
    "            self.a1 = tanh_function(z1)\n",
    "            z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "            self.a2 = tanh_function(z2)\n",
    "            z3 = np.dot(self.a2, self.w3) + self.b3\n",
    "            self.a3 = tanh_function(z3)\n",
    "            return self.a3\n",
    "        \n",
    "        elif self.activation_function == \"leakyrelu\":\n",
    "            z1 = np.dot(self.x, self.w1) + self.b1\n",
    "            self.a1 = leaky_relu_deravative(z1)\n",
    "            z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "            self.a2 = leaky_relu_deravative(z2)\n",
    "            z3 = np.dot(self.a2, self.w3) + self.b3\n",
    "            self.a3 = leaky_relu_deravative(z3)\n",
    "            return self.a3\n",
    "        \n",
    "    def backward_propagation(self):    \n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            dA3 = Error_found(self.y,self.a3) # w3\n",
    "            dZ2 = np.dot(dA3, self.w3.T)\n",
    "            dA2 = dZ2 * sigmoid_deravative(self.a2) # w2\n",
    "            dZ1 = np.dot(dA2, self.w2.T)\n",
    "            dA1 = dZ1 * sigmoid_deravative(self.a1) # w1\n",
    "        \n",
    "        elif self.activation_function == \"relu\":\n",
    "            dA3 = Error_found(self.y,self.a3) # w3\n",
    "            dZ2 = np.dot(dA3, self.w3.T)\n",
    "            dA2 = dZ2 * relu_deravative(self.a2) # w2\n",
    "            dZ1 = np.dot(dA2, self.w2.T)\n",
    "            dA1 = dZ1 * relu_deravative(self.a1) # w1\n",
    "        \n",
    "        elif self.activation_function == \"tanh\":\n",
    "            dA3 = Error_found(self.y,self.a3) # w3\n",
    "            dZ2 = np.dot(dA3, self.w3.T)\n",
    "            dA2 = dZ2 * tanh_deravative(self.a2) # w2\n",
    "            dZ1 = np.dot(dA2, self.w2.T)\n",
    "            dA1 = dZ1 * tanh_deravative(self.a1) # w1\n",
    "            \n",
    "        elif self.activation_function == \"leakyrelu\":\n",
    "            dA3 = Error_found(self.y,self.a3) # w3\n",
    "            dZ2 = np.dot(dA3, self.w3.T)\n",
    "            dA2 = dZ2 * tanh_deravative(self.a2) # w2\n",
    "            dZ1 = np.dot(dA2, self.w2.T)\n",
    "            dA1 = dZ1 * tanh_deravative(self.a1) # w1\n",
    "        \n",
    "\n",
    "        self.w3 -= np.dot(self.a2.T, dA3) * self.lr \n",
    "        self.b3 -= np.sum(dA3, axis=0) * self.lr \n",
    "        self.w2 -= np.dot(self.a1.T, dA2) * self.lr\n",
    "        self.b2 -= np.sum(dA2, axis=0) * self.lr\n",
    "        self.w1 -= np.dot(self.x.T, dA1) * self.lr\n",
    "        self.b1 -= np.sum(dA1, axis=0) * self.lr\n",
    "        \n",
    "    def predicted(self,x_test):\n",
    "        prediction = self.forward_propagation(x_test)\n",
    "        return prediction\n",
    "    \n",
    "dataset = pd.read_csv(\"circles500.csv\")\n",
    "X_ext = dataset.drop(columns = ['Class']).values\n",
    "y_ext = dataset.drop(columns = ['X0', 'X1']).values\n",
    "x_train_ext, x_test_ext, y_train_ext, y_test_ext = train_test_split(X_ext, y_ext, test_size=0.3, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Circle Data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circle Dataset: \n",
      "\n",
      "Confusion matrix :\n",
      " [[77  0]\n",
      " [ 1 72]]\n",
      "Train set Accuracy using  sigmoid  : 1.0\n",
      "Test  set Accuracy using :  sigmoid  : 0.9933333333333333\n",
      "========================================================================================================\n",
      "Confusion matrix :\n",
      " [[44 33]\n",
      " [ 7 66]]\n",
      "Train set Accuracy using  relu  : 0.8\n",
      "Test  set Accuracy using :  relu  : 0.7333333333333333\n",
      "========================================================================================================\n",
      "Confusion matrix :\n",
      " [[35 42]\n",
      " [ 1 72]]\n",
      "Train set Accuracy using  tanh  : 0.7742857142857142\n",
      "Test  set Accuracy using :  tanh  : 0.7133333333333334\n",
      "========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Circle Dataset: \\n\")\n",
    "activations = [\"sigmoid\", \"relu\", \"tanh\"]\n",
    "for i in activations:\n",
    "    model_3 = NeuralNetwork_model(x_train_ext, y_train_ext,lr= 0.001, hiddenlayer=6, activation_function = i)\n",
    "    iteration = 20000\n",
    "    for x in range(iteration):\n",
    "        model_3.forward_propagation(x_train_ext)\n",
    "        model_3.backward_propagation()\n",
    "\n",
    "    def prediction_3(x_test_ext):\n",
    "        pred = model_3.predicted(x_test_ext)\n",
    "        result =[]\n",
    "        for i in pred:\n",
    "            if i >= 0.5:\n",
    "                result.append(1)\n",
    "            elif i < 0.5:\n",
    "                result.append(0)\n",
    "        result = np.array(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test_ext, prediction_3(x_test_ext))\n",
    "    print(\"Confusion matrix :\\n\", cm)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    print(\"Train set Accuracy using \",i,  \" :\",accuracy_score(y_train_ext, prediction_3(x_train_ext)))\n",
    "    print(\"Test  set Accuracy using : \",i, \" :\",accuracy_score(y_test_ext, prediction_3(x_test_ext)))\n",
    "    print(\"========================================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation \n",
    "- We can observe that, for the Circle dataset, using sigmoid or relu as activation function is a better option. \n",
    "- However, tanh ended up predicting low on the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cifar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on Large CIFAR Dataset: \n",
      "\n",
      "Confusion matrix :\n",
      " [[187  90]\n",
      " [ 88 233]]\n",
      "Train set Accuracy for  sigmoid  : 0.6812634601579325\n",
      "Test  set Accuracy:  sigmoid  : 0.7023411371237458\n",
      "========================================================================================================\n",
      "Confusion matrix :\n",
      " [[200  77]\n",
      " [111 210]]\n",
      "Train set Accuracy for  relu  : 0.6805455850681982\n",
      "Test  set Accuracy:  relu  : 0.68561872909699\n",
      "========================================================================================================\n",
      "Confusion matrix :\n",
      " [[178  99]\n",
      " [ 78 243]]\n",
      "Train set Accuracy for  tanh  : 0.6884422110552764\n",
      "Test  set Accuracy:  tanh  : 0.7040133779264214\n",
      "========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Importing necessary libraries\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#SOURCE: loading code taken from \"code_snippets\" provided by Michael.Madden\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def loadbatch(batchname):\n",
    "    folder = 'cifar-10-batches-py'\n",
    "    batch = unpickle(folder+\"/\"+batchname)\n",
    "    return batch\n",
    "\n",
    "def loadlabelnames():\n",
    "    folder = 'cifar-10-batches-py'\n",
    "    meta = unpickle(folder+\"/\"+'batches.meta')\n",
    "    return meta[b'label_names']\n",
    "\n",
    "batch1 = loadbatch('data_batch_2')\n",
    "\n",
    "#Loads the data(features)\n",
    "data = batch1[b'data']\n",
    "#Loads the target variables\n",
    "labels = batch1[b'labels']\n",
    "\n",
    "#Converting to GreyScale\n",
    "Features = np.mean(data, axis=1)\n",
    "\n",
    "df = pd.DataFrame({'X':Features, 'Y': labels})\n",
    "\n",
    "indexNames = df[ (df['Y'] != 1) & (df['Y'] != 0) ].index #Filtering only bird and auto-mobile class\n",
    "df.drop(indexNames , inplace=True)\n",
    "\n",
    "XX = df.drop(['Y'], axis=1).values\n",
    "YY = df.drop(['X'], axis=1).values\n",
    "\n",
    "X_train_ext, X_test_ext, Y_train_ext, Y_test_ext = train_test_split(XX, YY, test_size=0.3)\n",
    "\n",
    "ss = StandardScaler() #normalize the data  \n",
    "\n",
    "X_train_ext = ss.fit_transform(X_train_ext)\n",
    "X_test_ext = ss.transform(X_test_ext)\n",
    "\n",
    "print(\"Performance on Large CIFAR Dataset: \\n\")\n",
    "activations = [\"sigmoid\", \"relu\", \"tanh\"]\n",
    "for i in activations:\n",
    "    model_3 = NeuralNetwork_model(X_train_ext, Y_train_ext,lr= 0.001, hiddenlayer=6, activation_function = i)\n",
    "    iteration = 20000\n",
    "    for x in range(iteration):\n",
    "        model_3.forward_propagation(X_train_ext)\n",
    "        model_3.backward_propagation()\n",
    "\n",
    "    def prediction_3(X_test_ext):\n",
    "        pred = model_3.predicted(X_test_ext)\n",
    "        result =[]\n",
    "        for i in pred:\n",
    "            if i >= 0.5:\n",
    "                result.append(1)\n",
    "            elif i < 0.5:\n",
    "                result.append(0)\n",
    "        result = np.array(result)\n",
    "        return result\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(Y_test_ext, prediction_3(X_test_ext))\n",
    "    print(\"Confusion matrix :\\n\", cm)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    print(\"Train set Accuracy for \",i,  \" :\",accuracy_score(Y_train_ext, prediction_3(X_train_ext)))\n",
    "    print(\"Test  set Accuracy: \",i, \" :\",accuracy_score(Y_test_ext, prediction_3(X_test_ext)))\n",
    "    print(\"========================================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "- [1] https://towardsdatascience.com/understanding-neural-networks-what-how-and-why-18ec703ebd31\n",
    "- [2] Andrew Ng- DeepLearning youtube. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
